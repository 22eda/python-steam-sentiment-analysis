

import requests
import pandas as pd
import time
from datetime import datetime
import json
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re

class SteamVeriCekici:
    def __init__(self):
        self.base_url = "https://store.steampowered.com/appreviews/"
        self.search_url = "https://store.steampowered.com/api/storesearch/"

        # Oyun listesi ve Steam App ID'leri
        self.oyun_listesi = {
            'The Last of Us Part I': 1888930,
            'Red Dead Redemption 2': 1174180,
            'Elden Ring': 1245620,
            'Cyberpunk 2077': 1091500,
            "Baldur's Gate 3": 1086940,
            'Hogwarts Legacy': 990080,
            'Stardew Valley': 413150,
            'The Witcher 3: Wild Hunt': 292030,
            'Detroit: Become Human': 1222140,
            "Marvel's Spider-Man": 1817070,
            'Hades': 1145360,
            'Resident Evil 4': 2050650,
            'Grand Theft Auto V': 271590,
            'The Sims 4': 1222670,
            'Counter-Strike 2': 730,
            'Dota 2': 570,
            'PUBG: BATTLEGROUNDS': 578080,
            'eFootball 2024': 1665460,
            'Battlefield 2042': 1517290,
            'Fallout 4': 377160,
            'Valheim':892970,
            'NBA 2K24': 2338770,
            'Gotham Knights': 1496790,
            'Call of Duty': 2519060,
            "Assassinâ€™s Creed Odyssey": 812140,
            'It Takes Two': 1426210,
            'Divinity: Original Sin 2': 435150,
            'Path of Exile': 238960,
            'ARK: Survival Evolved': 346110,
            'Sekiroâ„¢: Shadows Die Twice': 814380,
        }

    def oyun_id_bul(self, oyun_adi):
        """Oyun adÄ±ndan Steam App ID bulma (eÄŸer listede yoksa)"""
        params = {
            'term': oyun_adi,
            'l': 'english',
            'cc': 'US'
        }

        try:
            response = requests.get(self.search_url, params=params)
            if response.status_code == 200:
                data = response.json()
                if data.get('items'):
                    return data['items'][0]['id']
        except Exception as e:
            print(f"Oyun arama hatasÄ±: {e}")

        return None

    def yorumlari_cek(self, app_id, oyun_adi, hedef_yorum_sayisi=1000):
        """Belirli bir oyunun yorumlarÄ±nÄ± Steam API'den Ã§ek - Hedef sayÄ±ya ulaÅŸana kadar"""
        print(f"ğŸ“¥ {oyun_adi} iÃ§in {hedef_yorum_sayisi} yorum Ã§ekiliyor... (App ID: {app_id})")

        tum_yorumlar = []
        cursor = "*"
        sayfa = 0
        max_sayfa = 100  # GÃ¼venlik iÃ§in maksimum sayfa limiti

        while len(tum_yorumlar) < hedef_yorum_sayisi and sayfa < max_sayfa:
            url = f"{self.base_url}{app_id}"

            # Steam API parametreleri - daha fazla veri iÃ§in optimize edildi
            params = {
                'json': 1,
                'filter': 'all',        # TÃ¼m yorumlar (recent, helpful, funny, all)
                'language': 'all',      # TÃ¼m diller
                'day_range': 9999,      # TÃ¼m zamanlar (maksimum deÄŸer)
                'cursor': cursor,
                'review_type': 'all',   # TÃ¼m yorum tÃ¼rleri
                'purchase_type': 'all', # TÃ¼m satÄ±n alma tÃ¼rleri
                'num_per_page': 20      # Steam'in maksimum deÄŸeri
            }

            try:
                response = requests.get(url, params=params, timeout=15)

                if response.status_code == 200:
                    data = response.json()

                    if not data.get('success', False):
                        print(f"âŒ API hatasÄ±: {data.get('error', 'Bilinmeyen hata')}")
                        break

                    reviews = data.get('reviews', [])
                    if not reviews:
                        print(f"âš ï¸ {oyun_adi} - Sayfa {sayfa + 1}: Daha fazla yorum bulunamadÄ±")
                        break

                    # Yeni cursor deÄŸeri al
                    yeni_cursor = data.get('cursor')
                    if yeni_cursor == cursor or not yeni_cursor:
                        print(f"âš ï¸ {oyun_adi} - Cursor deÄŸiÅŸmedi, sonlandÄ±rÄ±lÄ±yor")
                        break

                    sayfa_yorum_sayisi = 0
                    for review in reviews:
                        try:
                            # Hedef sayÄ±ya ulaÅŸtÄ±ysak dur
                            if len(tum_yorumlar) >= hedef_yorum_sayisi:
                                break

                            yorum_data = {
                                'oyun_adi': oyun_adi,
                                'yorum_metni': review.get('review', '').strip(),
                                'begenildi_mi': review.get('voted_up', True),
                                'oyun_saati': review.get('author', {}).get('playtime_forever', 0),
                                'tarih': datetime.fromtimestamp(review.get('timestamp_created', 0)),
                                'yardimci_oy': review.get('votes_helpful', 0),
                                'komik_oy': review.get('votes_funny', 0),
                                'app_id': app_id,
                                'review_id': review.get('recommendationid', ''),
                                'dil': review.get('language', 'english'),
                                'yazar_oyun_sayisi': review.get('author', {}).get('num_games_owned', 0),
                                'yazar_yorum_sayisi': review.get('author', {}).get('num_reviews', 0)
                            }

                            # BoÅŸ ve Ã§ok kÄ±sa yorumlarÄ± filtrele
                            if len(yorum_data['yorum_metni']) > 5:
                                tum_yorumlar.append(yorum_data)
                                sayfa_yorum_sayisi += 1

                        except Exception as e:
                            print(f"Yorum iÅŸleme hatasÄ±: {e}")
                            continue

                    cursor = yeni_cursor
                    sayfa += 1

                    print(f"âœ… {oyun_adi} - Sayfa {sayfa}: {sayfa_yorum_sayisi} yorum Ã§ekildi "
                          f"(Toplam: {len(tum_yorumlar)}/{hedef_yorum_sayisi})")

                    # Hedef sayÄ±ya ulaÅŸtÄ±ysak dÃ¶ngÃ¼yÃ¼ kÄ±r
                    if len(tum_yorumlar) >= hedef_yorum_sayisi:
                        print(f"ğŸ¯ {oyun_adi} - Hedef sayÄ±ya ulaÅŸÄ±ldÄ±: {len(tum_yorumlar)} yorum")
                        break

                else:
                    print(f"âŒ HTTP HatasÄ± {response.status_code}")
                    if response.status_code == 429:  # Too Many Requests
                        print("â³ Rate limit aÅŸÄ±ldÄ±, 10 saniye bekleniyor...")
                        time.sleep(10)
                    break

                # Rate limiting - Steam'i yormamak iÃ§in
                time.sleep(1.2)  # Biraz daha hÄ±zlÄ± ama gÃ¼venli

            except requests.exceptions.Timeout:
                print(f"â° Timeout hatasÄ±, 5 saniye bekleyip tekrar denenecek...")
                time.sleep(5)
                continue
            except requests.exceptions.RequestException as e:
                print(f"âŒ Ä°stek hatasÄ±: {e}")
                time.sleep(5)
                break
            except Exception as e:
                print(f"âŒ Genel hata: {e}")
                break

        # DuplikatlarÄ± kaldÄ±r (aynÄ± review_id)
        onceki_sayfa = len(tum_yorumlar)
        unique_yorumlar = []
        gorulmus_idler = set()

        for yorum in tum_yorumlar:
            review_id = yorum.get('review_id', '')
            if review_id and review_id not in gorulmus_idler:
                unique_yorumlar.append(yorum)
                gorulmus_idler.add(review_id)
            elif not review_id:
                # ID yoksa yorum metnini kontrol et
                yorum_hash = hash(yorum['yorum_metni'] + yorum['oyun_adi'])
                if yorum_hash not in gorulmus_idler:
                    unique_yorumlar.append(yorum)
                    gorulmus_idler.add(yorum_hash)

        if len(unique_yorumlar) < onceki_sayfa:
            print(f"ğŸ”„ {onceki_sayfa - len(unique_yorumlar)} duplikat yorum kaldÄ±rÄ±ldÄ±")

        # Hedef sayÄ±dan fazla varsa kÄ±salt
        if len(unique_yorumlar) > hedef_yorum_sayisi:
            unique_yorumlar = unique_yorumlar[:hedef_yorum_sayisi]

        print(f"ğŸ¯ {oyun_adi} son durum: {len(unique_yorumlar)} yorum (Hedef: {hedef_yorum_sayisi})")
        return unique_yorumlar

    def tum_oyunlari_cek(self, kaydet=True, oyun_basi_yorum=1000):
        """TÃ¼m oyunlarÄ±n yorumlarÄ±nÄ± Ã§ek - Her oyundan belirli sayÄ±da"""
        print(f"ğŸ® Steam oyun yorumlarÄ± Ã§ekme iÅŸlemi baÅŸlÄ±yor... (Oyun baÅŸÄ± {oyun_basi_yorum} yorum)")
        print("=" * 80)

        tum_veriler = []
        basarili_oyunlar = []
        basarisiz_oyunlar = []
        toplam_oyun = len(self.oyun_listesi)
        mevcut_oyun = 0

        for oyun_adi, app_id in self.oyun_listesi.items():
            mevcut_oyun += 1
            print(f"\nğŸ¯ Ä°ÅŸleniyor: {oyun_adi} ({mevcut_oyun}/{toplam_oyun})")
            print("-" * 50)

            try:
                yorumlar = self.yorumlari_cek(app_id, oyun_adi, hedef_yorum_sayisi=oyun_basi_yorum)

                if yorumlar:
                    tum_veriler.extend(yorumlar)
                    basarili_oyunlar.append(f"{oyun_adi}: {len(yorumlar)} yorum")

                    # Her oyun sonrasÄ± ara kayÄ±t
                    if kaydet:
                        temp_df = pd.DataFrame(yorumlar)
                        temp_filename = f"temp_{oyun_adi.replace(':', '').replace(' ', '_').replace(chr(39), '')}.xlsx"
                        temp_df.to_excel(temp_filename, index=False, engine='openpyxl')
                        print(f"ğŸ’¾ Ara kayÄ±t: {temp_filename}")
                else:
                    basarisiz_oyunlar.append(oyun_adi)
                    print(f"âŒ {oyun_adi} - Yorum Ã§ekilemedi")

                # Oyunlar arasÄ± bekleme (Steam'e saygÄ±)
                if mevcut_oyun < toplam_oyun:
                    print("â³ Sonraki oyun iÃ§in 5 saniye bekleniyor...")
                    time.sleep(5)

            except Exception as e:
                print(f"âŒ {oyun_adi} kritik hatasÄ±: {e}")
                basarisiz_oyunlar.append(f"{oyun_adi}: {str(e)}")
                continue

        # SonuÃ§larÄ± DataFrame'e Ã§evir
        if tum_veriler:
            df = pd.DataFrame(tum_veriler)

            # Veri temizleme
            df = self.veri_temizle(df)

            if kaydet:
                # Ana dosyayÄ± kaydet
                timestamp = datetime.now().strftime('%Y%m%d_%H%M')
                dosya_adi = f"steam_yorumlar_{oyun_basi_yorum}per_game_{timestamp}.xlsx"
                df.to_excel(dosya_adi, index=False, engine='openpyxl')
                print(f"ğŸ’¾ Ana veri kaydedildi: {dosya_adi}")

                # JSON olarak da kaydet
                json_adi = dosya_adi.replace('.xlsx', '.json')
                df.to_json(json_adi, orient='records', date_format='iso', indent=2)
                print(f"ğŸ’¾ JSON kaydedildi: {json_adi}")

                # Temp dosyalarÄ± temizle
                try:
                    for file in os.listdir('.'):
                        if file.startswith('temp_') and file.endswith('.xlsx'):
                            os.remove(file)
                    print("ğŸ§¹ GeÃ§ici dosyalar temizlendi")
                except:
                    pass

            # DetaylÄ± Ã¶zet rapor
            self.ozet_rapor_yazdir(df, basarili_oyunlar, basarisiz_oyunlar, oyun_basi_yorum)

            return df
        else:
            print("âŒ HiÃ§ veri Ã§ekilemedi!")
            return None

    def ozet_rapor_yazdir(self, df, basarili_oyunlar, basarisiz_oyunlar, hedef_sayi):
        """DetaylÄ± Ã¶zet rapor yazdÄ±r"""
        print("\n" + "=" * 80)
        print("ğŸ“Š DETAYLI Ã–ZET RAPOR")
        print("=" * 80)

        print(f"ğŸ¯ HEDEF: Her oyundan {hedef_sayi} yorum")
        print(f"âœ… BaÅŸarÄ±lÄ± oyunlar ({len(basarili_oyunlar)}):")
        for oyun in basarili_oyunlar:
            print(f"   â€¢ {oyun}")

        if basarisiz_oyunlar:
            print(f"\nâŒ BaÅŸarÄ±sÄ±z oyunlar ({len(basarisiz_oyunlar)}):")
            for oyun in basarisiz_oyunlar:
                print(f"   â€¢ {oyun}")

        # Oyun bazlÄ± istatistikler
        oyun_stats = df.groupby('oyun_adi').agg({
            'yorum_metni': 'count',
            'begenildi_mi': ['mean', 'sum'],
            'oyun_saati_saat': 'mean',
            'yorum_uzunlugu': 'mean',
            'yardimci_oy': 'mean'
        }).round(2)

        print(f"\nğŸ“ˆ OYUN BAZLI Ä°STATÄ°STÄ°KLER:")
        print("-" * 80)
        for oyun in oyun_stats.index:
            stats = oyun_stats.loc[oyun]
            yorum_sayisi = int(stats[('yorum_metni', 'count')])
            pozitif_oran = stats[('begenildi_mi', 'mean')] * 100
            ortalama_saat = stats[('oyun_saati_saat', 'mean')]

            print(f"ğŸ® {oyun}")
            print(f"   Yorum sayÄ±sÄ±: {yorum_sayisi}")
            print(f"   Pozitif oran: %{pozitif_oran:.1f}")
            print(f"   Ort. oyun sÃ¼resi: {ortalama_saat:.1f} saat")
            print()

        print("=" * 80)
        print(f"ğŸ¯ GENEL TOPLAM:")
        print(f"   â€¢ Toplam yorum: {len(df):,}")
        print(f"   â€¢ Toplam oyun: {df['oyun_adi'].nunique()}")
        print(f"   â€¢ Pozitif yorum: {df['begenildi_mi'].sum():,} (%{df['begenildi_mi'].mean()*100:.1f})")
        print(f"   â€¢ Ortalama oyun sÃ¼resi: {df['oyun_saati_saat'].mean():.1f} saat")
        print(f"   â€¢ Ortalama yorum uzunluÄŸu: {df['yorum_uzunlugu'].mean():.0f} karakter")
        print(f"   â€¢ Tarih aralÄ±ÄŸÄ±: {df['tarih'].min()} - {df['tarih'].max()}")
        print(f"   â€¢ En Ã§ok yorumlanan: {df['oyun_adi'].value_counts().index[0]}")
        print("=" * 80)

    def veri_temizle(self, df):
        """Ã‡ekilen veriyi temizle ve standardize et"""
        print("\nğŸ§¹ Veriler temizleniyor...")

        # BoÅŸ deÄŸerleri temizle
        df = df.dropna(subset=['yorum_metni'])
        df = df[df['yorum_metni'].str.len() > 5]

        # DuplikatlarÄ± kaldÄ±r (yorum metni + oyun adÄ± kombinasyonu)
        onceki_boyut = len(df)
        df = df.drop_duplicates(subset=['yorum_metni', 'oyun_adi'])
        print(f"   â€¢ {onceki_boyut - len(df)} duplikat kaldÄ±rÄ±ldÄ±")

        # Veri tiplerini dÃ¼zelt
        df['oyun_saati'] = pd.to_numeric(df['oyun_saati'], errors='coerce').fillna(0)
        df['yardimci_oy'] = pd.to_numeric(df['yardimci_oy'], errors='coerce').fillna(0)
        df['komik_oy'] = pd.to_numeric(df['komik_oy'], errors='coerce').fillna(0)
        df['yazar_oyun_sayisi'] = pd.to_numeric(df['yazar_oyun_sayisi'], errors='coerce').fillna(0)
        df['yazar_yorum_sayisi'] = pd.to_numeric(df['yazar_yorum_sayisi'], errors='coerce').fillna(0)

        # Tarih sÃ¼tununu dÃ¼zelt
        df['tarih'] = pd.to_datetime(df['tarih'], errors='coerce')

        # Yorum uzunluÄŸu sÃ¼tunu ekle
        df['yorum_uzunlugu'] = df['yorum_metni'].str.len()

        # Oyun saatini saat cinsine Ã§evir (dakikadan)
        df['oyun_saati_saat'] = (df['oyun_saati'] / 60).round(1)

        # Kategorik sÃ¼tunlar ekle
        df['yorum_uzunluk_kategori'] = pd.cut(df['yorum_uzunlugu'],
                                            bins=[0, 50, 150, 500, float('inf')],
                                            labels=['KÄ±sa', 'Orta', 'Uzun', 'Ã‡ok Uzun'])

        df['oyun_deneyim_kategori'] = pd.cut(df['oyun_saati_saat'],
                                           bins=[0, 1, 10, 50, 100, float('inf')],
                                           labels=['Yeni', 'Az', 'Orta', 'Ã‡ok', 'Uzman'])

        print(f"âœ… Temizlik tamamlandÄ±. Final veri: {len(df)} yorum")
        return df

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    # Veri Ã§ekici oluÅŸtur
    cekici = SteamVeriCekici()

    # TÃ¼m oyunlarÄ±n verilerini Ã§ek - Her oyundan 1000 yorum
    print("ğŸš€ Steam veri Ã§ekme iÅŸlemi baÅŸlatÄ±lÄ±yor...")
    print("âš ï¸  Bu iÅŸlem uzun sÃ¼rebilir (yaklaÅŸÄ±k 2-3 saat)")
    print("ğŸ’¡ Her oyundan 1000 yorum hedefleniyor")

    df = cekici.tum_oyunlari_cek(kaydet=True, oyun_basi_yorum=1000)

    if df is not None:
        print("\nğŸ‰ Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!")
        print(f"ğŸ“ Dosyalar oluÅŸturuldu:")
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        print(f"   â€¢ Excel: steam_yorumlar_1000per_game_{timestamp}.xlsx")
        print(f"   â€¢ JSON: steam_yorumlar_1000per_game_{timestamp}.json")

        print(f"\nğŸ† BAÅARIYLA TAMAMLANDI!")
        print(f"   â€¢ Hedeflenen toplam: {len(cekici.oyun_listesi) * 1000:,} yorum")
        print(f"   â€¢ GerÃ§ekleÅŸen toplam: {len(df):,} yorum")
        print(f"   â€¢ BaÅŸarÄ± oranÄ±: %{(len(df) / (len(cekici.oyun_listesi) * 1000)) * 100:.1f}")
    else:
        print("âŒ Veri Ã§ekme iÅŸlemi baÅŸarÄ±sÄ±z!")



        from google.colab import files
files.download('steam_yorumlar_1000per_game_20250531_2301.xlsx')

import pandas as pd
import numpy as np
import re
from langdetect import detect, DetectorFactory
from langdetect.lang_detect_exception import LangDetectException
import warnings
warnings.filterwarnings('ignore')

# Dil algÄ±lama tutarlÄ±lÄ±ÄŸÄ± iÃ§in seed ayarla
DetectorFactory.seed = 0

def clean_text(text):
    """Metni temizle ve normalize et"""
    if pd.isna(text) or text == '':
        return ''
    
    # String'e Ã§evir
    text = str(text)
    
    # HTML etiketlerini kaldÄ±r
    text = re.sub(r'<[^>]+>', '', text)
    
    # URL'leri kaldÄ±r
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # Steam emoticon'larÄ±nÄ± kaldÄ±r (:steamhappy:, :steamsad: vs.)
    text = re.sub(r':[a-zA-Z0-9_]+:', '', text)
    
    # Fazla boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
    text = re.sub(r'\s+', ' ', text)
    
    # BaÅŸ ve sondaki boÅŸluklarÄ± kaldÄ±r
    text = text.strip()
    
    return text

def detect_language(text):
    """Metnin dilini algÄ±la"""
    try:
        if pd.isna(text) or text == '' or len(text.strip()) < 3:
            return 'unknown'
        
        # TemizlenmiÅŸ metni kullan
        cleaned_text = clean_text(text)
        if len(cleaned_text.strip()) < 3:
            return 'unknown'
            
        detected_lang = detect(cleaned_text)
        return detected_lang
    except (LangDetectException, Exception):
        return 'unknown'

def process_steam_reviews(file_path, output_path=None):
    """Steam yorumlarÄ±nÄ± iÅŸle ve temizle"""
    
    print("Excel dosyasÄ± okunuyor...")
    try:
        df = pd.read_excel( file_path )
    except Exception as e:
        print(f"Dosya okuma hatasÄ±: {e}")
        return None
    
    print(f"Toplam veri sayÄ±sÄ±: {len(df)}")
    print(f"SÃ¼tunlar: {list(df.columns)}")
    
    # Eksik deÄŸerleri kontrol et
    print(f"\nEksik deÄŸer sayÄ±larÄ±:")
    print(df.isnull().sum())
    
    # Yorum metni boÅŸ olanlarÄ± kaldÄ±r
    initial_count = len(df)
    df = df.dropna(subset=['yorum_metni'])
    df = df[df['yorum_metni'].str.strip() != '']
    print(f"\nBoÅŸ yorumlar Ã§Ä±karÄ±ldÄ±ktan sonra: {len(df)} veri ({initial_count - len(df)} veri Ã§Ä±karÄ±ldÄ±)")
    
    # Dil algÄ±lama
    print("\nDil algÄ±lama yapÄ±lÄ±yor...")
    df['detected_language'] = df['yorum_metni'].apply(detect_language)
    
    # Dil daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
    print("\nDil daÄŸÄ±lÄ±mÄ±:")
    print(df['detected_language'].value_counts())
    
    # Sadece Ä°ngilizce yorumlarÄ± filtrele
    english_df = df[df['detected_language'] == 'en'].copy()
    print(f"\nÄ°ngilizce yorumlar: {len(english_df)} veri")
    
    # Yorum metinlerini temizle
    print("Metin temizleme yapÄ±lÄ±yor...")
    english_df['yorum_metni_temiz'] = english_df['yorum_metni'].apply(clean_text)
    
    # Temizlendikten sonra Ã§ok kÄ±sa kalan yorumlarÄ± Ã§Ä±kar (3 kelimeden az)
    english_df['kelime_sayisi'] = english_df['yorum_metni_temiz'].apply(lambda x: len(str(x).split()))
    english_df = english_df[english_df['kelime_sayisi'] >= 3]
    print(f"KÄ±sa yorumlar Ã§Ä±karÄ±ldÄ±ktan sonra: {len(english_df)} veri")
    
    # Duplicate yorumlarÄ± kaldÄ±r
    before_dedup = len(english_df)
    english_df = english_df.drop_duplicates(subset=['yorum_metni_temiz'])
    print(f"Duplicate yorumlar Ã§Ä±karÄ±ldÄ±ktan sonra: {len(english_df)} veri ({before_dedup - len(english_df)} duplicate Ã§Ä±karÄ±ldÄ±)")
    
    # Veri tiplerini dÃ¼zenle
    if 'begenildi_mi' in english_df.columns:
        english_df['begenildi_mi'] = english_df['begenildi_mi'].astype(bool)
    
    if 'oyun_saati' in english_df.columns:
        english_df['oyun_saati'] = pd.to_numeric(english_df['oyun_saati'], errors='coerce')
    
    # Tarih sÃ¼tununu datetime'a Ã§evir
    if 'tarih' in english_df.columns:
        english_df['tarih'] = pd.to_datetime(english_df['tarih'], errors='coerce')
    
    # Ä°statistikleri gÃ¶ster
    print(f"\n=== TEMÄ°ZLENMÄ°Å VERÄ° Ä°STATÄ°STÄ°KLERÄ° ===")
    print(f"Toplam Ä°ngilizce yorum: {len(english_df)}")
    print(f"Ortalama yorum uzunluÄŸu: {english_df['yorum_uzunlugu'].mean():.1f} karakter")
    print(f"Ortalama kelime sayÄ±sÄ±: {english_df['kelime_sayisi'].mean():.1f} kelime")
    
    if 'begenildi_mi' in english_df.columns:
        print(f"Pozitif yorumlar: {english_df['begenildi_mi'].sum()} (%{english_df['begenildi_mi'].mean()*100:.1f})")
        print(f"Negatif yorumlar: {(~english_df['begenildi_mi']).sum()} (%{(~english_df['begenildi_mi']).mean()*100:.1f})")
    
    # DosyayÄ± kaydet
    if output_path is None:
        output_path = file_path.replace('.xlsx', '_temizlenmis.xlsx')
    
    print(f"\nTemizlenmiÅŸ veri kaydediliyor: {output_path}")
    english_df.to_excel(output_path, index=False)
    
    return english_df

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    # Dosya yolunu buraya yazÄ±n
    file_path = "a.xlsx"  # Kendi dosya yolunuzu yazÄ±n
    
    # Veri temizleme iÅŸlemini baÅŸlat
    cleaned_df = process_steam_reviews(file_path)
    
    if cleaned_df is not None:
        print("\n=== Ä°ÅLEM TAMAMLANDI ===")
        print("Duygu analizi iÃ§in hazÄ±r!")
        
        # Ã–rnek temizlenmiÅŸ yorumlarÄ± gÃ¶ster
        print("\n--- Ã–rnek TemizlenmiÅŸ Yorumlar ---")
        for i, row in cleaned_df.head(3).iterrows():
            print(f"\nOyun: {row['oyun_adi']}")
            print(f"Orijinal: {row['yorum_metni'][:100]}...")
            print(f"TemizlenmiÅŸ: {row['yorum_metni_temiz'][:100]}...")
            print(f"BeÄŸenildi mi: {row['begenildi_mi']}")

            import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

# NLTK verilerini indir (ilk Ã§alÄ±ÅŸtÄ±rmada gerekli)
def download_nltk_data():
    """NLTK verilerini indir"""
    nltk_downloads = [
        'punkt',
        'punkt_tab', 
        'stopwords',
        'wordnet',
        'omw-1.4',
        'averaged_perceptron_tagger'
    ]
    
    for item in nltk_downloads:
        try:
            nltk.data.find(f'tokenizers/{item}')
        except LookupError:
            try:
                print(f"Ä°ndiriliyor: {item}")
                nltk.download(item, quiet=True)
            except:
                print(f"Ä°ndirilemedi: {item} (atlanÄ±yor)")
        except:
            try:
                print(f"Ä°ndiriliyor: {item}")
                nltk.download(item, quiet=True)
            except:
                print(f"Ä°ndirilemedi: {item} (atlanÄ±yor)")

# NLTK verilerini indir
download_nltk_data()

class SteamSentimentAnalyzer:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        self.vader_analyzer = SentimentIntensityAnalyzer()
        
        # Gaming-specific stop words ekle
        gaming_stopwords = {'game', 'play', 'playing', 'played', 'steam', 'review', 
                           'recommend', 'good', 'bad', 'like', 'really', 'much', 
                           'would', 'could', 'one', 'get', 'also', 'even', 'still',
                            "fucking","bf","ass","dont","isn","ea","im","fuck","shit",
                        "fallout","aminake","bolas", "ve", "don", "didn", "doesn", 
                        "ll", "elden", "stardew", "spider", "ps", "dlc", "pc","the", 
                        "and", "it","games","played","playing","hours","lot","feels",
                        "gameplay","characters","harry","batman","potter","sims","hades",
                        "witcher","battlefield","left","dota","yeah","lots","times","arkham",
                        "fighting","enjoyed ","runs","de","ive","cant"}
        self.stop_words.update(gaming_stopwords)
    
    def preprocess_text(self, text):
        """Metni Ã¶n iÅŸleme"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        
        # Ã–zel karakterleri kaldÄ±r, sadece harfler ve boÅŸluk bÄ±rak
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # Basit tokenize et (NLTK baÄŸÄ±mlÄ±lÄ±ÄŸÄ±nÄ± azalt)
        try:
            tokens = word_tokenize(text)
        except:
            # NLTK sorun yaÅŸarsa basit split kullan
            tokens = text.split()
        
        # Stop words ve kÄ±sa kelimeleri kaldÄ±r, lemmatize et
        processed_tokens = []
        for token in tokens:
            if (token not in self.stop_words and 
                len(token) > 2 and 
                token.isalpha()):
                try:
                    lemmatized = self.lemmatizer.lemmatize(token)
                    processed_tokens.append(lemmatized)
                except:
                    # Lemmatizer sorun yaÅŸarsa orijinal kelimeyi kullan
                    processed_tokens.append(token)
        
        return ' '.join(processed_tokens)
    
    def get_word_frequency(self, texts, top_n=50):
        """Kelime frekansÄ± analizi"""
        all_words = []
        for text in texts:
            if text:
                words = text.split()
                all_words.extend(words)
        
        word_freq = Counter(all_words)
        return word_freq.most_common(top_n)
    
    def analyze_sentiment_textblob(self, text):
        """TextBlob ile duygu analizi"""
        if pd.isna(text) or text == '':
            return {'polarity': 0, 'subjectivity': 0, 'sentiment': 'neutral'}
        
        blob = TextBlob(str(text))
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        
        if polarity > 0.1:
            sentiment = 'positive'
        elif polarity < -0.1:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        return {
            'polarity': polarity,
            'subjectivity': subjectivity,
            'sentiment': sentiment
        }
    
    def analyze_sentiment_vader(self, text):
        """VADER ile duygu analizi"""
        if pd.isna(text) or text == '':
            return {'compound': 0, 'pos': 0, 'neu': 1, 'neg': 0, 'sentiment': 'neutral'}
        
        scores = self.vader_analyzer.polarity_scores(str(text))
        
        if scores['compound'] >= 0.05:
            sentiment = 'positive'
        elif scores['compound'] <= -0.05:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        scores['sentiment'] = sentiment
        return scores
    
    def create_wordcloud(self, text_data, title="Word Cloud", figsize=(12, 8)):
        """Kelime bulutu oluÅŸtur"""
        if not text_data:
            print("Kelime bulutu iÃ§in yeterli veri yok")
            return
        
        # TÃ¼m metinleri birleÅŸtir
        all_text = ' '.join([str(text) for text in text_data if str(text).strip()])
        
        if not all_text.strip():
            print("Kelime bulutu iÃ§in yeterli veri yok")
            return
        
        wordcloud = WordCloud(
            width=1200, height=600,
            background_color='white',
            max_words=100,
            colormap='viridis'
        ).generate(all_text)
        
        plt.figure(figsize=figsize)
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(title, fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    def plot_sentiment_distribution(self, df):
        """Duygu daÄŸÄ±lÄ±mÄ± grafikleri"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # TextBlob Sentiment Distribution
        sentiment_counts = df['textblob_sentiment'].value_counts()
        axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')
        axes[0, 0].set_title('TextBlob Duygu DaÄŸÄ±lÄ±mÄ±')
        
        # VADER Sentiment Distribution
        vader_counts = df['vader_sentiment'].value_counts()
        axes[0, 1].pie(vader_counts.values, labels=vader_counts.index, autopct='%1.1f%%')
        axes[0, 1].set_title('VADER Duygu DaÄŸÄ±lÄ±mÄ±')
        
        # Steam Recommendation vs Sentiment
        if 'begenildi_mi' in df.columns:
            recommendation_sentiment = pd.crosstab(df['begenildi_mi'], df['textblob_sentiment'])
            recommendation_sentiment.plot(kind='bar', ax=axes[0, 2])
            axes[0, 2].set_title('Steam Tavsiyesi vs TextBlob Duygu')
            axes[0, 2].set_xlabel('Steam Tavsiyesi')
            axes[0, 2].legend(title='Duygu')
        
        # Polarity Distribution
        axes[1, 0].hist(df['textblob_polarity'], bins=30, alpha=0.7, color='blue')
        axes[1, 0].set_title('TextBlob Polarity DaÄŸÄ±lÄ±mÄ±')
        axes[1, 0].set_xlabel('Polarity')
        
        # Compound Score Distribution
        axes[1, 1].hist(df['vader_compound'], bins=30, alpha=0.7, color='green')
        axes[1, 1].set_title('VADER Compound Score DaÄŸÄ±lÄ±mÄ±')
        axes[1, 1].set_xlabel('Compound Score')
        
        # Game Hours vs Sentiment
        if 'oyun_saati' in df.columns:
            df_filtered = df[df['oyun_saati'] < 1000]  # Outlier'larÄ± filtrele
            sentiment_hours = df_filtered.groupby('textblob_sentiment')['oyun_saati'].mean()
            axes[1, 2].bar(sentiment_hours.index, sentiment_hours.values)
            axes[1, 2].set_title('Ortalama Oyun Saati vs Duygu')
            axes[1, 2].set_xlabel('Duygu')
            axes[1, 2].set_ylabel('Ortalama Saat')
        
        plt.tight_layout()
        plt.show()
    
    def plot_word_frequency(self, word_freq, title="En SÄ±k KullanÄ±lan Kelimeler", top_n=20):
        """Kelime frekansÄ± grafiÄŸi"""
        if not word_freq:
            print("Kelime frekansÄ± verisi yok")
            return
        
        words, counts = zip(*word_freq[:top_n])
        
        plt.figure(figsize=(12, 8))
        bars = plt.barh(range(len(words)), counts)
        plt.yticks(range(len(words)), words)
        plt.xlabel('Frekans')
        plt.title(title)
        plt.gca().invert_yaxis()
        
        # Bar'lara deÄŸerleri ekle
        for i, bar in enumerate(bars):
            plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, 
                    str(counts[i]), va='center')
        
        plt.tight_layout()
        plt.show()
    
    def analyze_reviews(self, df, text_column='yorum_metni_temiz'):
        """Ana analiz fonksiyonu"""
        print("Duygu analizi baÅŸlÄ±yor...")
        
        # Metinleri Ã¶n iÅŸleme
        print("Metin Ã¶n iÅŸleme yapÄ±lÄ±yor...")
        df['processed_text'] = df[text_column].apply(self.preprocess_text)
        
        # TextBlob analizi
        print("TextBlob duygu analizi yapÄ±lÄ±yor...")
        textblob_results = df[text_column].apply(self.analyze_sentiment_textblob)
        df['textblob_polarity'] = [r['polarity'] for r in textblob_results]
        df['textblob_subjectivity'] = [r['subjectivity'] for r in textblob_results]
        df['textblob_sentiment'] = [r['sentiment'] for r in textblob_results]
        
        # VADER analizi
        print("VADER duygu analizi yapÄ±lÄ±yor...")
        vader_results = df[text_column].apply(self.analyze_sentiment_vader)
        df['vader_compound'] = [r['compound'] for r in vader_results]
        df['vader_pos'] = [r['pos'] for r in vader_results]
        df['vader_neu'] = [r['neu'] for r in vader_results]
        df['vader_neg'] = [r['neg'] for r in vader_results]
        df['vader_sentiment'] = [r['sentiment'] for r in vader_results]
        
        # Kelime frekansÄ± analizi
        print("Kelime frekansÄ± analizi yapÄ±lÄ±yor...")
        
        # Genel kelime frekansÄ±
        all_word_freq = self.get_word_frequency(df['processed_text'].tolist())
        
        # Pozitif yorumlar iÃ§in kelime frekansÄ±
        positive_texts = df[df['textblob_sentiment'] == 'positive']['processed_text'].tolist()
        positive_word_freq = self.get_word_frequency(positive_texts)
        
        # Negatif yorumlar iÃ§in kelime frekansÄ±
        negative_texts = df[df['textblob_sentiment'] == 'negative']['processed_text'].tolist()
        negative_word_freq = self.get_word_frequency(negative_texts)
        
        # SonuÃ§larÄ± gÃ¶ster
        print(f"\n=== DUYGU ANALÄ°ZÄ° SONUÃ‡LARI ===")
        print(f"Toplam analiz edilen yorum: {len(df)}")
        
        print("\n--- TextBlob SonuÃ§larÄ± ---")
        textblob_counts = df['textblob_sentiment'].value_counts()
        for sentiment, count in textblob_counts.items():
            print(f"{sentiment.capitalize()}: {count} (%{count/len(df)*100:.1f})")
        
        print("\n--- VADER SonuÃ§larÄ± ---")
        vader_counts = df['vader_sentiment'].value_counts()
        for sentiment, count in vader_counts.items():
            print(f"{sentiment.capitalize()}: {count} (%{count/len(df)*100:.1f})")
        
        # Steam tavsiyesi ile karÅŸÄ±laÅŸtÄ±r
        if 'begenildi_mi' in df.columns:
            print("\n--- Steam Tavsiyesi vs Analiz SonuÃ§larÄ± ---")
            steam_positive = df['begenildi_mi'].sum()
            steam_negative = len(df) - steam_positive
            print(f"Steam Pozitif Tavsiye: {steam_positive} (%{steam_positive/len(df)*100:.1f})")
            print(f"Steam Negatif Tavsiye: {steam_negative} (%{steam_negative/len(df)*100:.1f})")
        
        # Grafikler
        self.plot_sentiment_distribution(df)
        
        # Kelime frekansÄ± grafikleri
        self.plot_word_frequency(all_word_freq, "Genel En SÄ±k KullanÄ±lan Kelimeler")
        
        if positive_word_freq:
            self.plot_word_frequency(positive_word_freq, "Pozitif Yorumlarda En SÄ±k Kelimeler")
        
        if negative_word_freq:
            self.plot_word_frequency(negative_word_freq, "Negatif Yorumlarda En SÄ±k Kelimeler")
        
        # Kelime bulutlarÄ±
        print("Kelime bulutlarÄ± oluÅŸturuluyor...")
        self.create_wordcloud(df['processed_text'].tolist(), "Genel Kelime Bulutu")
        
        if positive_texts:
            self.create_wordcloud(positive_texts, "Pozitif Yorumlar Kelime Bulutu")
        
        if negative_texts:
            self.create_wordcloud(negative_texts, "Negatif Yorumlar Kelime Bulutu")
        
        return df, {
            'all_words': all_word_freq,
            'positive_words': positive_word_freq,
            'negative_words': negative_word_freq
        }

# Ana kullanÄ±m fonksiyonu
def main():
    # TemizlenmiÅŸ veriyi yÃ¼kle
    file_path = "a_temizlenmis.xlsx"  # Dosya yolunu buraya yazÄ±n
    
    print("Veri yÃ¼kleniyor...")
    try:
        df = pd.read_excel(file_path)
        print(f"Veri baÅŸarÄ±yla yÃ¼klendi: {len(df)} yorum")
    except Exception as e:
        print(f"Dosya yÃ¼kleme hatasÄ±: {e}")
        return
    
    # Analyzer'Ä± baÅŸlat
    analyzer = SteamSentimentAnalyzer()
    
    # Analiz yap
    analyzed_df, word_frequencies = analyzer.analyze_reviews(df)
    
    # SonuÃ§larÄ± kaydet
    output_path = file_path.replace('.xlsx', '_analiz_sonuclari.xlsx')
    analyzed_df.to_excel(output_path, index=False)
    print(f"\nAnaliz sonuÃ§larÄ± kaydedildi: {output_path}")
    
    # En sÄ±k kullanÄ±lan kelimeleri yazdÄ±r
    print(f"\n=== EN SIK KULLANILAN KELÄ°MELER ===")
    print("Genel:")
    for word, count in word_frequencies['all_words'][:10]:
        print(f"  {word}: {count}")
    
    if word_frequencies['positive_words']:
        print("\nPozitif yorumlarda:")
        for word, count in word_frequencies['positive_words'][:10]:
            print(f"  {word}: {count}")
    
    if word_frequencies['negative_words']:
        print("\nNegatif yorumlarda:")
        for word, count in word_frequencies['negative_words'][:10]:
            print(f"  {word}: {count}")

if __name__ == "__main__":
    main()